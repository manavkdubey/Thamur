# ğŸ—¿ Thamur: The Rust Web Crawler

## ğŸ“– Overview
This is a **multi-threaded, async web crawler** built in Rust. It efficiently fetches and parses web pages while respecting `robots.txt` rules, handling rate limits, and storing crawled data. The project is modular, making it **scalable and extensible**.

## ğŸ¯ Features
âœ… **Asynchronous Crawling** using `tokio`
âœ… **Concurrency Management** with a custom thread pool (`flume` + `tokio::task`)
âœ… **Rate Limiting** via a **token bucket** strategy
âœ… **Respects robots.txt** and obeys crawl restrictions
âœ… **Efficient URL Validation** using regex
âœ… **Stores crawled data** in JSON format
âœ… **Error Handling** with `thiserror`
âœ… **Modular Design** for extensibility

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs          # Entry point, manages tasks
â”‚   â”œâ”€â”€ client.rs        # Fetches web pages (reqwest)
â”‚   â”œâ”€â”€ parser.rs        # Extracts links from HTML (select.rs)
â”‚   â”œâ”€â”€ robot.rs         # Parses robots.txt
â”‚   â”œâ”€â”€ limiter.rs       # Implements rate limiting
â”‚   â”œâ”€â”€ storage.rs       # Stores crawled data in JSON
â”‚   â”œâ”€â”€ config.rs        # Loads config settings
â”‚   â”œâ”€â”€ thread.rs        # Custom thread pool
â”‚   â”œâ”€â”€ task.rs          # Task execution
â”‚   â”œâ”€â”€ state.rs         # Manages visited URLs & queue
â”‚   â”œâ”€â”€ validator.rs     # URL validation with regex
â”‚   â”œâ”€â”€ error.rs         # Centralized error handling
â”‚   â”œâ”€â”€ lib.rs           # Module declarations
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ fetcher/
â”‚       â”œâ”€â”€ parser/
â”‚       â”œâ”€â”€ storage/
â”‚       â”œâ”€â”€ utils/
â”‚       â”œâ”€â”€ thread/
â”‚       â”œâ”€â”€ validator/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ Cargo.toml           # Dependencies & metadata
â””â”€â”€ README.md            # Project documentation
```

## âš™ï¸ Installation
Ensure you have **Rust** installed (via `rustup`).
```sh
# Clone this repository
git clone https://github.com/manavkdubey/Thamur.git
cd rust-web-crawler

# Build the project
cargo build --release
```

## ğŸš€ Usage
Run the crawler with:
```sh
cargo run --release
```
Or specify a URL to crawl:
```sh
cargo run --release -- http://example.com
```

## ğŸ› ï¸ Configuration
Modify `config.json`:
```json
{
  "user_agent": "MyCrawler/1.0",
  "max_depth": 3,
  "max_threads": 5,
  "timeout": 10
}
```

## ğŸ—ï¸ Contributing
1. **Fork** this repo
2. **Clone** your fork
3. **Create a new branch**
4. **Commit & push** changes
5. **Open a pull request** ğŸš€

## ğŸ“œ License
This project is licensed under the **MIT License**.

---
Happy Crawling! ğŸ•·ï¸ğŸš€
